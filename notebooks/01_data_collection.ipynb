{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ShohnomaLLM - Сбор данных\n",
        "\n",
        "Этот notebook собирает таджикскую поэзию из различных источников:\n",
        "- **Ganjoor.net** - классическая персидская поэзия (с транслитерацией)\n",
        "- **Adabiyot.tj** - современная таджикская поэзия\n",
        "\n",
        "## Инструкции\n",
        "1. Запустите все ячейки последовательно\n",
        "2. Данные будут сохранены в Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Подключение Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Создаём директорию проекта\n",
        "!mkdir -p /content/drive/MyDrive/ShohnomaLLM/data/raw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Установка зависимостей\n",
        "!pip install requests beautifulsoup4 lxml tqdm -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Клонируем репозиторий (если есть на GitHub)\n",
        "# !git clone https://github.com/username/ShohnomaLLM.git\n",
        "\n",
        "# Или копируем код напрямую\n",
        "import os\n",
        "os.chdir('/content')\n",
        "!mkdir -p ShohnomaLLM/scraper/utils ShohnomaLLM/scraper/sources"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Транслитератор\n",
        "Конвертация персидской арабицы в таджикскую кириллицу"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile /content/ShohnomaLLM/scraper/utils/transliterate.py\n",
        "\n",
        "import re\n",
        "import json\n",
        "from typing import Optional\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "class PersianToTajikTransliterator:\n",
        "    CONSONANTS = {\n",
        "        'ب': 'б', 'پ': 'п', 'ت': 'т', 'ث': 'с',\n",
        "        'ج': 'ҷ', 'چ': 'ч', 'ح': 'ҳ', 'خ': 'х',\n",
        "        'د': 'д', 'ذ': 'з', 'ر': 'р', 'ز': 'з',\n",
        "        'ژ': 'ж', 'س': 'с', 'ش': 'ш', 'ص': 'с',\n",
        "        'ض': 'з', 'ط': 'т', 'ظ': 'з', 'ع': 'ъ',\n",
        "        'غ': 'ғ', 'ف': 'ф', 'ق': 'қ', 'ک': 'к',\n",
        "        'ك': 'к', 'گ': 'г', 'ل': 'л', 'م': 'м',\n",
        "        'ن': 'н', 'ه': 'ҳ', 'ی': 'й', 'ي': 'й',\n",
        "    }\n",
        "    \n",
        "    WORD_DICT = {\n",
        "        'من': 'ман', 'تو': 'ту', 'او': 'ӯ', 'ما': 'мо',\n",
        "        'است': 'аст', 'بود': 'буд', 'شد': 'шуд',\n",
        "        'دل': 'дил', 'جان': 'ҷон', 'عشق': 'ишқ',\n",
        "        'و': 'ва', 'که': 'ки', 'از': 'аз', 'به': 'ба',\n",
        "        'در': 'дар', 'با': 'бо', 'بر': 'бар',\n",
        "    }\n",
        "    \n",
        "    def __init__(self):\n",
        "        pass\n",
        "    \n",
        "    def transliterate(self, text: str) -> str:\n",
        "        if not text:\n",
        "            return \"\"\n",
        "        text = self._normalize(text)\n",
        "        tokens = self._tokenize(text)\n",
        "        result = []\n",
        "        for token in tokens:\n",
        "            if self._is_persian(token):\n",
        "                result.append(self._transliterate_word(token))\n",
        "            else:\n",
        "                result.append(token)\n",
        "        return ''.join(result)\n",
        "    \n",
        "    def _normalize(self, text: str) -> str:\n",
        "        text = text.replace('ي', 'ی').replace('ك', 'ک')\n",
        "        return text\n",
        "    \n",
        "    def _tokenize(self, text: str) -> list:\n",
        "        pattern = r'([\\u0600-\\u06FF]+|[^\\u0600-\\u06FF]+)'\n",
        "        return re.findall(pattern, text)\n",
        "    \n",
        "    def _is_persian(self, token: str) -> bool:\n",
        "        return bool(re.match(r'^[\\u0600-\\u06FF]+$', token))\n",
        "    \n",
        "    def _transliterate_word(self, word: str) -> str:\n",
        "        if word in self.WORD_DICT:\n",
        "            return self.WORD_DICT[word]\n",
        "        return self._apply_rules(word)\n",
        "    \n",
        "    def _apply_rules(self, word: str) -> str:\n",
        "        result = []\n",
        "        for i, char in enumerate(word):\n",
        "            if char == 'ا':\n",
        "                result.append('о' if i > 0 else '')\n",
        "            elif char == 'آ':\n",
        "                result.append('о')\n",
        "            elif char == 'و':\n",
        "                result.append('у' if i > 0 else 'в')\n",
        "            elif char in ('ی', 'ي'):\n",
        "                result.append('ӣ' if i == len(word)-1 else 'и')\n",
        "            elif char == 'ه' and i == len(word)-1:\n",
        "                result.append('а')\n",
        "            elif char == 'ع' and i == 0:\n",
        "                result.append('')\n",
        "            elif char in self.CONSONANTS:\n",
        "                result.append(self.CONSONANTS[char])\n",
        "            else:\n",
        "                result.append(char)\n",
        "        return ''.join(result)\n",
        "    \n",
        "    def transliterate_poem(self, poem: str) -> str:\n",
        "        lines = poem.split('\\n')\n",
        "        return '\\n'.join(self.transliterate(line) for line in lines)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Парсер Ganjoor.net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "\n",
        "# Импортируем транслитератор\n",
        "import sys\n",
        "sys.path.insert(0, '/content/ShohnomaLLM')\n",
        "from scraper.utils.transliterate import PersianToTajikTransliterator\n",
        "\n",
        "\n",
        "class GanjoorScraper:\n",
        "    BASE_URL = \"https://api.ganjoor.net/api\"\n",
        "    \n",
        "    POETS = {\n",
        "        2: \"Рӯдакӣ\",\n",
        "        5: \"Хайём\",\n",
        "        7: \"Ҳофиз\",\n",
        "        22: \"Саъдӣ\",\n",
        "        26: \"Мавлавӣ\",\n",
        "        35: \"Ҷомӣ\",\n",
        "    }\n",
        "    \n",
        "    FORM_MAP = {\n",
        "        'رباعیات': 'rubaiyat', 'رباعی': 'rubaiyat',\n",
        "        'غزلیات': 'ghazal', 'غزل': 'ghazal',\n",
        "        'قصاید': 'qasida', 'مثنوی': 'masnavi',\n",
        "    }\n",
        "    \n",
        "    def __init__(self, output_dir):\n",
        "        self.output_dir = Path(output_dir)\n",
        "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
        "        self.transliterator = PersianToTajikTransliterator()\n",
        "        self.session = requests.Session()\n",
        "    \n",
        "    def _request(self, endpoint):\n",
        "        try:\n",
        "            resp = self.session.get(f\"{self.BASE_URL}/{endpoint}\", timeout=30)\n",
        "            return resp.json() if resp.ok else None\n",
        "        except:\n",
        "            return None\n",
        "    \n",
        "    def get_categories(self, poet_id):\n",
        "        data = self._request(f\"ganjoor/poet/{poet_id}\")\n",
        "        if not data or 'cat' not in data:\n",
        "            return []\n",
        "        return self._extract_cats(data['cat'])\n",
        "    \n",
        "    def _extract_cats(self, cat, cats=None):\n",
        "        if cats is None:\n",
        "            cats = []\n",
        "        for child in cat.get('children', []):\n",
        "            cats.append({'id': child['id'], 'title': child['title']})\n",
        "            self._extract_cats(child, cats)\n",
        "        return cats\n",
        "    \n",
        "    def get_poems(self, cat_id):\n",
        "        data = self._request(f\"ganjoor/cat/{cat_id}?poems=true\")\n",
        "        if data and 'cat' in data:\n",
        "            return data['cat'].get('poems', [])\n",
        "        return []\n",
        "    \n",
        "    def get_poem_text(self, poem_id):\n",
        "        data = self._request(f\"ganjoor/poem/{poem_id}?verseDetails=true\")\n",
        "        if data and 'verses' in data:\n",
        "            return '\\n'.join(v['text'] for v in data['verses'] if 'text' in v)\n",
        "        return ''\n",
        "    \n",
        "    def _detect_form(self, title):\n",
        "        for pattern, form in self.FORM_MAP.items():\n",
        "            if pattern in title:\n",
        "                return form\n",
        "        return 'other'\n",
        "    \n",
        "    def scrape_poet(self, poet_id):\n",
        "        poet_name = self.POETS.get(poet_id, f\"Poet_{poet_id}\")\n",
        "        print(f\"\\nСбор: {poet_name}\")\n",
        "        \n",
        "        poems = []\n",
        "        categories = self.get_categories(poet_id)\n",
        "        \n",
        "        for cat in tqdm(categories, desc=\"Категории\"):\n",
        "            cat_poems = self.get_poems(cat['id'])\n",
        "            \n",
        "            for p in cat_poems:\n",
        "                text_fa = self.get_poem_text(p['id'])\n",
        "                if not text_fa:\n",
        "                    continue\n",
        "                \n",
        "                text_tj = self.transliterator.transliterate_poem(text_fa)\n",
        "                \n",
        "                poems.append({\n",
        "                    'id': f\"ganjoor_{poet_id}_{p['id']}\",\n",
        "                    'poet': poet_name,\n",
        "                    'title': p.get('title', ''),\n",
        "                    'text_persian': text_fa,\n",
        "                    'text_tajik': text_tj,\n",
        "                    'form': self._detect_form(cat['title']),\n",
        "                    'source': 'ganjoor',\n",
        "                })\n",
        "                time.sleep(0.1)\n",
        "        \n",
        "        return poems\n",
        "    \n",
        "    def save(self, poems, filename):\n",
        "        path = self.output_dir / filename\n",
        "        with open(path, 'w', encoding='utf-8') as f:\n",
        "            for p in poems:\n",
        "                json.dump(p, f, ensure_ascii=False)\n",
        "                f.write('\\n')\n",
        "        print(f\"Сохранено: {len(poems)} -> {path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Запуск сбора данных"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Сбор классической поэзии\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/ShohnomaLLM/data/raw/ganjoor\"\n",
        "\n",
        "scraper = GanjoorScraper(OUTPUT_DIR)\n",
        "\n",
        "all_poems = []\n",
        "\n",
        "# Собираем по каждому поэту\n",
        "for poet_id in [5, 7]:  # Начнём с Хайяма и Хафиза\n",
        "    poems = scraper.scrape_poet(poet_id)\n",
        "    all_poems.extend(poems)\n",
        "    scraper.save(poems, f\"poet_{poet_id}.jsonl\")\n",
        "\n",
        "# Сохраняем всё вместе\n",
        "scraper.save(all_poems, \"all_classical.jsonl\")\n",
        "\n",
        "print(f\"\\nВсего собрано: {len(all_poems)} стихов\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Просмотр примера\n",
        "if all_poems:\n",
        "    poem = all_poems[0]\n",
        "    print(\"Пример:\")\n",
        "    print(f\"Поэт: {poem['poet']}\")\n",
        "    print(f\"Форма: {poem['form']}\")\n",
        "    print(f\"\\nОригинал:\\n{poem['text_persian'][:200]}\")\n",
        "    print(f\"\\nТранслитерация:\\n{poem['text_tajik'][:200]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Статистика"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Статистика по формам\n",
        "forms = Counter(p['form'] for p in all_poems)\n",
        "print(\"Статистика по формам:\")\n",
        "for form, count in forms.most_common():\n",
        "    print(f\"  {form}: {count}\")\n",
        "\n",
        "# Статистика по поэтам\n",
        "poets = Counter(p['poet'] for p in all_poems)\n",
        "print(\"\\nСтатистика по поэтам:\")\n",
        "for poet, count in poets.most_common():\n",
        "    print(f\"  {poet}: {count}\")"
      ]
    }
  ]
}

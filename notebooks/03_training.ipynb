{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ShohnomaLLM - Обучение модели\n",
        "\n",
        "Fine-tuning Qwen2.5-1.5B для генерации таджикских стихов.\n",
        "\n",
        "**Требования:**\n",
        "- GPU: T4 (15GB VRAM) или лучше\n",
        "- Данные в Google Drive (из notebook 01)\n",
        "\n",
        "**Время обучения:** ~2-4 часа на T4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Проверка GPU\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Установка Unsloth (оптимизированный fine-tuning)\n",
        "%%capture\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install --no-deps trl peft accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Подключение Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "PROJECT_DIR = \"/content/drive/MyDrive/ShohnomaLLM\"\n",
        "DATA_DIR = f\"{PROJECT_DIR}/data\"\n",
        "MODEL_DIR = f\"{PROJECT_DIR}/models\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Подготовка данных"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import random\n",
        "from pathlib import Path\n",
        "\n",
        "# Системный промпт\n",
        "SYSTEM_PROMPT = \"\"\"Ту шоири тоҷикӣ ҳастӣ. Ту метавонӣ шеърҳои классикӣ (рубоӣ, ғазал, қасида) ва шеърҳои озод бинависӣ.\"\"\"\n",
        "\n",
        "# Промпты по формам\n",
        "PROMPTS = {\n",
        "    'rubaiyat': ['Рубоӣ бинавис', 'Як рубоӣ эҷод кун', 'Чор мисраъ бинавис'],\n",
        "    'ghazal': ['Ғазал бинавис', 'Ғазали ошиқона эҷод кун'],\n",
        "    'qasida': ['Қасида бинавис'],\n",
        "    'masnavi': ['Маснавӣ бинавис'],\n",
        "    'other': ['Шеър бинавис', 'Шеъри зебо эҷод кун'],\n",
        "}\n",
        "\n",
        "def format_example(poem):\n",
        "    \"\"\"Форматирование в ChatML\"\"\"\n",
        "    text = poem.get('text_tajik') or poem.get('text', '')\n",
        "    form = poem.get('form', 'other')\n",
        "    prompt = random.choice(PROMPTS.get(form, PROMPTS['other']))\n",
        "    \n",
        "    return {\n",
        "        'text': f\"\"\"<|im_start|>system\n",
        "{SYSTEM_PROMPT}<|im_end|>\n",
        "<|im_start|>user\n",
        "{prompt}<|im_end|>\n",
        "<|im_start|>assistant\n",
        "{text}<|im_end|>\"\"\"\n",
        "    }\n",
        "\n",
        "# Загрузка и форматирование данных\n",
        "def load_and_format(input_path):\n",
        "    examples = []\n",
        "    with open(input_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            poem = json.loads(line)\n",
        "            text = poem.get('text_tajik') or poem.get('text', '')\n",
        "            if len(text) > 20:\n",
        "                examples.append(format_example(poem))\n",
        "    return examples\n",
        "\n",
        "# Загружаем данные\n",
        "data_file = f\"{DATA_DIR}/raw/ganjoor/all_classical.jsonl\"\n",
        "examples = load_and_format(data_file)\n",
        "\n",
        "print(f\"Загружено примеров: {len(examples)}\")\n",
        "print(f\"\\nПример:\\n{examples[0]['text'][:500]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Разбиение на train/val\n",
        "random.seed(42)\n",
        "random.shuffle(examples)\n",
        "\n",
        "val_size = int(len(examples) * 0.1)\n",
        "train_examples = examples[val_size:]\n",
        "val_examples = examples[:val_size]\n",
        "\n",
        "print(f\"Train: {len(train_examples)}\")\n",
        "print(f\"Val: {len(val_examples)}\")\n",
        "\n",
        "# Сохраняем\n",
        "Path(f\"{DATA_DIR}/training\").mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "with open(f\"{DATA_DIR}/training/train.jsonl\", 'w') as f:\n",
        "    for ex in train_examples:\n",
        "        json.dump(ex, f, ensure_ascii=False)\n",
        "        f.write('\\n')\n",
        "\n",
        "with open(f\"{DATA_DIR}/training/val.jsonl\", 'w') as f:\n",
        "    for ex in val_examples:\n",
        "        json.dump(ex, f, ensure_ascii=False)\n",
        "        f.write('\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Загрузка модели"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "# Конфигурация\n",
        "MODEL_NAME = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
        "MAX_SEQ_LENGTH = 512\n",
        "\n",
        "# Загрузка модели с 4-bit квантизацией\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=MODEL_NAME,\n",
        "    max_seq_length=MAX_SEQ_LENGTH,\n",
        "    dtype=None,  # Auto-detect\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "\n",
        "print(f\"Модель загружена: {MODEL_NAME}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Добавляем LoRA адаптеры\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=32,  # LoRA rank\n",
        "    target_modules=[\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "    ],\n",
        "    lora_alpha=64,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=\"unsloth\",\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "print(\"LoRA адаптеры добавлены\")\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Обучение"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "# Загрузка датасета\n",
        "dataset = load_dataset(\n",
        "    'json',\n",
        "    data_files={\n",
        "        'train': f\"{DATA_DIR}/training/train.jsonl\",\n",
        "        'validation': f\"{DATA_DIR}/training/val.jsonl\",\n",
        "    }\n",
        ")\n",
        "\n",
        "print(f\"Train: {len(dataset['train'])}\")\n",
        "print(f\"Val: {len(dataset['validation'])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Настройка обучения\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset['train'],\n",
        "    eval_dataset=dataset['validation'],\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=MAX_SEQ_LENGTH,\n",
        "    args=TrainingArguments(\n",
        "        output_dir=\"./outputs\",\n",
        "        \n",
        "        # Batch size\n",
        "        per_device_train_batch_size=4,\n",
        "        per_device_eval_batch_size=4,\n",
        "        gradient_accumulation_steps=8,\n",
        "        \n",
        "        # Learning rate\n",
        "        learning_rate=2e-4,\n",
        "        lr_scheduler_type=\"cosine\",\n",
        "        warmup_ratio=0.1,\n",
        "        \n",
        "        # Epochs\n",
        "        num_train_epochs=3,\n",
        "        \n",
        "        # Optimization\n",
        "        bf16=True,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        max_grad_norm=1.0,\n",
        "        \n",
        "        # Logging\n",
        "        logging_steps=10,\n",
        "        eval_steps=100,\n",
        "        evaluation_strategy=\"steps\",\n",
        "        \n",
        "        # Saving\n",
        "        save_strategy=\"epoch\",\n",
        "        save_total_limit=2,\n",
        "        load_best_model_at_end=True,\n",
        "        \n",
        "        # Other\n",
        "        seed=42,\n",
        "        report_to=\"none\",\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Запуск обучения\n",
        "print(\"Начало обучения...\")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Сохранение модели"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Сохраняем LoRA адаптеры\n",
        "lora_path = f\"{MODEL_DIR}/tajik-poetry-lora\"\n",
        "model.save_pretrained(lora_path)\n",
        "tokenizer.save_pretrained(lora_path)\n",
        "print(f\"LoRA сохранён: {lora_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Объединяем LoRA с базовой моделью\n",
        "merged_path = f\"{MODEL_DIR}/tajik-poetry-1.5b\"\n",
        "\n",
        "model.save_pretrained_merged(\n",
        "    merged_path,\n",
        "    tokenizer,\n",
        "    save_method=\"merged_16bit\",\n",
        ")\n",
        "print(f\"Merged модель сохранена: {merged_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Тестирование"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Включаем режим inference\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "def generate_poem(prompt, max_tokens=256, temperature=0.8):\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "        {\"role\": \"user\", \"content\": prompt},\n",
        "    ]\n",
        "    \n",
        "    text = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True,\n",
        "    )\n",
        "    \n",
        "    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
        "    \n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_tokens,\n",
        "        temperature=temperature,\n",
        "        top_p=0.9,\n",
        "        repetition_penalty=1.1,\n",
        "        do_sample=True,\n",
        "    )\n",
        "    \n",
        "    response = tokenizer.decode(\n",
        "        outputs[0][inputs.input_ids.shape[1]:],\n",
        "        skip_special_tokens=True,\n",
        "    )\n",
        "    \n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Генерация примеров\n",
        "prompts = [\n",
        "    \"Рубоӣ бинавис\",\n",
        "    \"Ғазали ошиқона эҷод кун\",\n",
        "    \"Шеър дар бораи баҳор бинавис\",\n",
        "]\n",
        "\n",
        "for prompt in prompts:\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Запрос: {prompt}\")\n",
        "    print(f\"{'='*50}\")\n",
        "    poem = generate_poem(prompt)\n",
        "    print(poem)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Экспорт в GGUF (опционально)\n",
        "\n",
        "Для запуска на CPU через llama.cpp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Экспорт в GGUF (4-bit квантизация)\n",
        "gguf_path = f\"{MODEL_DIR}/tajik-poetry-1.5b-q4_k_m.gguf\"\n",
        "\n",
        "model.save_pretrained_gguf(\n",
        "    gguf_path.replace('.gguf', ''),\n",
        "    tokenizer,\n",
        "    quantization_method=\"q4_k_m\",\n",
        ")\n",
        "print(f\"GGUF сохранён: {gguf_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Обучение завершено!\")\n",
        "print(\"=\"*50)\n",
        "print(f\"\\nМодели сохранены в: {MODEL_DIR}\")\n",
        "print(\"\\nДля использования:\")\n",
        "print(\"1. Скачайте папку models/ из Google Drive\")\n",
        "print(\"2. Используйте inference/generator.py\")"
      ]
    }
  ]
}

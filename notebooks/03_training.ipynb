{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# ShohnomaLLM - Обучение модели\n\nFine-tuning **Qwen3-4B** для генерации персидской поэзии (арабица).\n\n**Настройки:**\n- ✅ Обучаем на **оригинальной арабице** (не транслитерация!)\n- ✅ 30K лучших стихов (отфильтровано из 119K)\n- ✅ 1 эпоха (достаточно для 30K)\n\n**Время: ~15-20 минут на A100**"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проверка GPU\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Установка зависимостей\n",
    "%%capture\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install --no-deps trl peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Клонируем репозиторий ShohnomaLLM\n",
    "!git clone https://github.com/Kuchizu/ShohnomaLLM.git\n",
    "%cd ShohnomaLLM\n",
    "!pip install -r requirements.txt -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Подключение Google Drive (для сохранения модели)\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\n# Пути\nREPO_DIR = \"/content/ShohnomaLLM\"\nDATA_DIR = f\"{REPO_DIR}/data\"  # Данные из репозитория\nMODEL_DIR = \"/content/drive/MyDrive/ShohnomaLLM/models\"  # Модели в Drive\n\n# Создаём директории\n!mkdir -p {DATA_DIR}/processed {DATA_DIR}/training {MODEL_DIR}"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Подготовка данных\n",
    "\n",
    "Используем модули из репозитория"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nsys.path.insert(0, REPO_DIR)\n\nfrom training.format_dataset import DatasetFormatter\nfrom training.config import get_config\n\n# Загружаем конфигурацию для A100\nconfig = get_config(\"colab_a100\")\nprint(f\"Модель: {config.model.base_model}\")\nprint(f\"LoRA rank: {config.lora.r}\")\nprint(f\"Batch size: {config.training.per_device_train_batch_size}\")\nprint(f\"Gradient accumulation: {config.training.gradient_accumulation_steps}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Загрузка данных с HuggingFace (БЕЗ транслитерации - используем оригинал!)\nimport os\nimport json\nfrom tqdm import tqdm\n\nhf_data = f\"{DATA_DIR}/raw/ganjoor_hf/all_poems.jsonl\"\n\n# Если данных нет - скачиваем\nif not os.path.exists(hf_data):\n    print(\"Загрузка датасета с HuggingFace (119K стихов)...\")\n    from datasets import load_dataset\n    \n    os.makedirs(f\"{DATA_DIR}/raw/ganjoor_hf\", exist_ok=True)\n    \n    dataset_hf = load_dataset(\"mabidan/ganjoor\", split=\"train\")\n    print(f\"Загружено: {len(dataset_hf)} стихов\")\n    \n    # Сохраняем БЕЗ транслитерации - оригинальный персидский!\n    print(\"Сохранение оригинальных текстов (арабица)...\")\n    with open(hf_data, 'w', encoding='utf-8') as f:\n        for item in tqdm(dataset_hf, desc=\"Обработка\"):\n            text = item.get(\"text\", \"\")\n            if not text or len(text) < 30:\n                continue\n            \n            poem_data = {\n                \"id\": f\"hf_{item.get('id', 0)}\",\n                \"poet\": item.get(\"poet\", \"\"),\n                \"text_persian\": text,  # Оригинал!\n                \"form\": \"other\",\n                \"source\": \"huggingface\",\n            }\n            json.dump(poem_data, f, ensure_ascii=False)\n            f.write('\\n')\n    \n    print(f\"Сохранено: {hf_data}\")\n\n# Подготовка датасета\nfrom training.format_dataset import prepare_full_dataset\n\nstats = prepare_full_dataset(\n    raw_dir=f\"{DATA_DIR}/raw\",\n    processed_dir=f\"{DATA_DIR}/processed\",\n    training_dir=f\"{DATA_DIR}/training\",\n)\nprint(f\"\\nВсего подготовлено: {stats.get('total', 0)} примеров\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Загрузка модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "# Загрузка модели\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=config.model.base_model,\n",
    "    max_seq_length=config.model.max_seq_length,\n",
    "    dtype=None,\n",
    "    load_in_4bit=config.model.load_in_4bit,\n",
    ")\n",
    "\n",
    "print(f\"Модель загружена: {config.model.base_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Добавляем LoRA (БЕЗ gradient checkpointing для скорости!)\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r=config.lora.r,\n    target_modules=config.lora.target_modules,\n    lora_alpha=config.lora.lora_alpha,\n    lora_dropout=config.lora.lora_dropout,\n    bias=config.lora.bias,\n    use_gradient_checkpointing=False,  # ВАЖНО: выключено для скорости на A100!\n    random_state=42,\n)\n\nprint(\"LoRA добавлен (без gradient checkpointing)\")\nmodel.print_trainable_parameters()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from datasets import load_dataset\n\n# Лимит данных для быстрого обучения\nMAX_SAMPLES = 30000\n\n# Загрузка датасета\ndataset = load_dataset(\n    'json',\n    data_files={\n        'train': f\"{DATA_DIR}/training/train.jsonl\",\n        'validation': f\"{DATA_DIR}/training/val.jsonl\",\n    }\n)\n\nprint(f\"Загружено - Train: {len(dataset['train'])}, Val: {len(dataset['validation'])}\")\n\n# Ограничиваем размер для скорости\nif len(dataset['train']) > MAX_SAMPLES:\n    dataset['train'] = dataset['train'].shuffle(seed=42).select(range(MAX_SAMPLES))\n    dataset['validation'] = dataset['validation'].shuffle(seed=42).select(range(min(3000, len(dataset['validation']))))\n    print(f\"Ограничено до {MAX_SAMPLES} примеров для обучения\")\n\nprint(f\"Итого - Train: {len(dataset['train'])}, Val: {len(dataset['validation'])}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Настройка параметров - ОПТИМИЗИРОВАНО ДЛЯ A100 + 30K данных\nfrom trl import SFTTrainer, SFTConfig\n\ndataset_size = len(dataset['train'])\nprint(f\"Размер датасета: {dataset_size}\")\n\n# 1 эпоха для 30K данных\nnum_epochs = 1\ntotal_steps = dataset_size // 32  # batch=8 x accum=4 = 32\neval_steps = max(100, total_steps // 5)\nsave_steps = eval_steps\n\nprint(f\"Epochs: {num_epochs}\")\nprint(f\"Ожидаемых шагов: ~{total_steps}\")\nprint(f\"Ожидаемое время: ~{total_steps // 60 + 5}-{total_steps // 45 + 5} минут\")\n\n# Trainer с PACKING\ntrainer = SFTTrainer(\n    model=model,\n    processing_class=tokenizer,\n    train_dataset=dataset['train'],\n    eval_dataset=dataset['validation'],\n    \n    args=SFTConfig(\n        output_dir=\"./outputs\",\n        dataset_text_field=\"text\",\n        \n        # === PACKING ===\n        packing=True,\n        max_seq_length=512,  # Уменьшено для экономии памяти\n        dataset_num_proc=4,\n        \n        # === BATCH SIZE для A100 (оптимизировано) ===\n        per_device_train_batch_size=8,\n        per_device_eval_batch_size=8,\n        gradient_accumulation_steps=4,  # Эффективный batch = 32\n        \n        # === Learning Rate ===\n        learning_rate=2e-4,\n        lr_scheduler_type=\"cosine\",\n        warmup_ratio=0.05,\n        num_train_epochs=num_epochs,\n        \n        # === Precision ===\n        bf16=True,\n        optim=\"adamw_8bit\",\n        weight_decay=0.01,\n        max_grad_norm=1.0,\n        \n        # === Dataloader ===\n        dataloader_num_workers=4,\n        dataloader_pin_memory=True,\n        \n        # === Logging ===\n        logging_steps=25,\n        eval_steps=eval_steps,\n        eval_strategy=\"steps\",\n        save_steps=save_steps,\n        save_strategy=\"steps\",\n        save_total_limit=2,\n        load_best_model_at_end=True,\n        \n        seed=42,\n        report_to=\"none\",\n    ),\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Запуск обучения\n",
    "print(\"Начало обучения...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Сохранение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохраняем LoRA\n",
    "lora_path = f\"{MODEL_DIR}/tajik-poetry-lora\"\n",
    "model.save_pretrained(lora_path)\n",
    "tokenizer.save_pretrained(lora_path)\n",
    "print(f\"LoRA сохранён: {lora_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Объединяем LoRA с базовой моделью (16-bit)\nmerged_path = f\"{MODEL_DIR}/tajik-poetry-4b\"\n\nmodel.save_pretrained_merged(\n    merged_path,\n    tokenizer,\n    save_method=\"merged_16bit\",\n)\nprint(f\"Merged модель: {merged_path}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Тестирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Режим inference\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Системный промпт\n",
    "from training.format_dataset import SYSTEM_PROMPT\n",
    "\n",
    "def generate_poem(prompt, max_tokens=256, temperature=0.8):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "    \n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "        top_p=0.9,\n",
    "        repetition_penalty=1.1,\n",
    "        do_sample=True,\n",
    "    )\n",
    "    \n",
    "    return tokenizer.decode(\n",
    "        outputs[0][inputs.input_ids.shape[1]:],\n",
    "        skip_special_tokens=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Тестовые промпты (персидский)\ntest_prompts = [\n    \"رباعی بنویس\",           # Напиши рубаи\n    \"غزل عاشقانه بساز\",      # Создай любовную газель\n    \"شعر درباره بهار بنویس\",  # Напиши стих о весне\n]\n\nfor prompt in test_prompts:\n    print(f\"\\n{'='*50}\")\n    print(f\"Запрос: {prompt}\")\n    print(f\"{'='*50}\")\n    print(generate_poem(prompt))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Экспорт в GGUF (опционально)\n",
    "\n",
    "Для запуска на CPU через llama.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Экспорт в GGUF\n",
    "gguf_path = f\"{MODEL_DIR}/tajik-poetry-q4\"\n",
    "\n",
    "model.save_pretrained_gguf(\n",
    "    gguf_path,\n",
    "    tokenizer,\n",
    "    quantization_method=\"q4_k_m\",\n",
    ")\n",
    "print(f\"GGUF сохранён: {gguf_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\" + \"=\"*50)\nprint(\"Обучение завершено!\")\nprint(\"=\"*50)\nprint(f\"\\nМодели сохранены в Google Drive: {MODEL_DIR}\")\nprint(\"\\nСкачайте модель и используйте локально:\")\nprint(\"  python -m cli.generate --model путь/к/модели\")"
  }
 ]
}
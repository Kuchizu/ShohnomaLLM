{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# ShohnomaLLM - Обучение модели\n\nFine-tuning Qwen2.5-1.5B для генерации таджикских стихов.\n\n**Данные:** ~50,000-80,000 стихов от 203 поэтов (HuggingFace)\n\n**GPU:** Выберите A100 для быстрого обучения (~1 час)\n\nRuntime → Change runtime type → **A100 GPU**"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проверка GPU\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Установка зависимостей\n",
    "%%capture\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install --no-deps trl peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Клонируем репозиторий ShohnomaLLM\n",
    "!git clone https://github.com/Kuchizu/ShohnomaLLM.git\n",
    "%cd ShohnomaLLM\n",
    "!pip install -r requirements.txt -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Подключение Google Drive (для сохранения модели)\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\n# Пути\nREPO_DIR = \"/content/ShohnomaLLM\"\nDATA_DIR = f\"{REPO_DIR}/data\"  # Данные из репозитория\nMODEL_DIR = \"/content/drive/MyDrive/ShohnomaLLM/models\"  # Модели в Drive\n\n# Создаём директории\n!mkdir -p {DATA_DIR}/processed {DATA_DIR}/training {MODEL_DIR}"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Подготовка данных\n",
    "\n",
    "Используем модули из репозитория"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nsys.path.insert(0, REPO_DIR)\n\nfrom training.format_dataset import DatasetFormatter\nfrom training.config import get_config\n\n# Загружаем конфигурацию\nconfig = get_config(\"colab_t4\")  # или \"colab_a100\" для A100\nprint(f\"Модель: {config.model.base_model}\")\nprint(f\"LoRA rank: {config.lora.r}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Загрузка данных с HuggingFace (если нет локальных)\nimport os\nimport json\nfrom tqdm import tqdm\n\nlocal_data = f\"{DATA_DIR}/raw/ganjoor/all_classical.jsonl\"\nhf_data = f\"{DATA_DIR}/raw/ganjoor_hf/all_poems.jsonl\"\n\n# Если HF данных нет - скачиваем\nif not os.path.exists(hf_data):\n    print(\"Загрузка датасета с HuggingFace (119K стихов)...\")\n    from datasets import load_dataset\n    \n    # Создаём директорию\n    os.makedirs(f\"{DATA_DIR}/raw/ganjoor_hf\", exist_ok=True)\n    \n    # Загружаем датасет\n    dataset_hf = load_dataset(\"mabidan/ganjoor\", split=\"train\")\n    print(f\"Загружено: {len(dataset_hf)} стихов\")\n    \n    # Инициализируем транслитератор\n    from scraper.utils.transliterate import PersianToTajikTransliterator\n    transliterator = PersianToTajikTransliterator()\n    \n    # Обрабатываем и сохраняем\n    print(\"Транслитерация в таджикскую кириллицу...\")\n    with open(hf_data, 'w', encoding='utf-8') as f:\n        for item in tqdm(dataset_hf, desc=\"Обработка\"):\n            text = item.get(\"text\", \"\")\n            if not text or len(text) < 30:\n                continue\n            \n            text_tajik = transliterator.transliterate_poem(text)\n            \n            poem_data = {\n                \"id\": f\"hf_{item.get('id', 0)}\",\n                \"poet\": item.get(\"poet\", \"\"),\n                \"text_tajik\": text_tajik,\n                \"form\": \"other\",\n                \"source\": \"huggingface\",\n            }\n            json.dump(poem_data, f, ensure_ascii=False)\n            f.write('\\n')\n    \n    print(f\"Сохранено: {hf_data}\")\n\n# Подготовка датасета\nfrom training.format_dataset import prepare_full_dataset\n\nstats = prepare_full_dataset(\n    raw_dir=f\"{DATA_DIR}/raw\",\n    processed_dir=f\"{DATA_DIR}/processed\",\n    training_dir=f\"{DATA_DIR}/training\",\n)\nprint(f\"\\nВсего подготовлено: {stats.get('total', 0)} примеров\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Загрузка модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "# Загрузка модели\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=config.model.base_model,\n",
    "    max_seq_length=config.model.max_seq_length,\n",
    "    dtype=None,\n",
    "    load_in_4bit=config.model.load_in_4bit,\n",
    ")\n",
    "\n",
    "print(f\"Модель загружена: {config.model.base_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Добавляем LoRA\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=config.lora.r,\n",
    "    target_modules=config.lora.target_modules,\n",
    "    lora_alpha=config.lora.lora_alpha,\n",
    "    lora_dropout=config.lora.lora_dropout,\n",
    "    bias=config.lora.bias,\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "print(\"LoRA добавлен\")\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "# Загрузка датасета\n",
    "dataset = load_dataset(\n",
    "    'json',\n",
    "    data_files={\n",
    "        'train': f\"{DATA_DIR}/training/train.jsonl\",\n",
    "        'validation': f\"{DATA_DIR}/training/val.jsonl\",\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(dataset['train'])}\")\n",
    "print(f\"Val: {len(dataset['validation'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Настройка параметров для большого датасета\ndataset_size = len(dataset['train'])\nprint(f\"Размер датасета: {dataset_size}\")\n\n# Автоматическая настройка epochs\nif dataset_size > 30000:\n    num_epochs = 2\n    eval_steps = 500\n    save_steps = 500\nelif dataset_size > 10000:\n    num_epochs = 3\n    eval_steps = 300\n    save_steps = 300\nelse:\n    num_epochs = 5\n    eval_steps = 100\n    save_steps = 100\n\nprint(f\"Epochs: {num_epochs}, Eval/Save steps: {eval_steps}\")\n\n# Trainer (оптимизировано для A100)\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=dataset['train'],\n    eval_dataset=dataset['validation'],\n    dataset_text_field=\"text\",\n    max_seq_length=1024,\n    args=TrainingArguments(\n        output_dir=\"./outputs\",\n        per_device_train_batch_size=8,   # A100: 8, T4: 4\n        per_device_eval_batch_size=8,\n        gradient_accumulation_steps=4,    # A100: 4, T4: 8\n        learning_rate=2e-4,\n        lr_scheduler_type=\"cosine\",\n        warmup_ratio=0.1,\n        num_train_epochs=num_epochs,\n        bf16=True,\n        optim=\"adamw_8bit\",\n        weight_decay=0.01,\n        max_grad_norm=1.0,\n        logging_steps=10,\n        eval_steps=eval_steps,\n        eval_strategy=\"steps\",\n        save_steps=save_steps,\n        save_strategy=\"steps\",\n        save_total_limit=2,\n        load_best_model_at_end=True,\n        seed=42,\n        report_to=\"none\",\n    ),\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Запуск обучения\n",
    "print(\"Начало обучения...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Сохранение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохраняем LoRA\n",
    "lora_path = f\"{MODEL_DIR}/tajik-poetry-lora\"\n",
    "model.save_pretrained(lora_path)\n",
    "tokenizer.save_pretrained(lora_path)\n",
    "print(f\"LoRA сохранён: {lora_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Объединяем LoRA с базовой моделью (16-bit)\n",
    "merged_path = f\"{MODEL_DIR}/tajik-poetry-1.5b\"\n",
    "\n",
    "model.save_pretrained_merged(\n",
    "    merged_path,\n",
    "    tokenizer,\n",
    "    save_method=\"merged_16bit\",\n",
    ")\n",
    "print(f\"Merged модель: {merged_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Тестирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Режим inference\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Системный промпт\n",
    "from training.format_dataset import SYSTEM_PROMPT\n",
    "\n",
    "def generate_poem(prompt, max_tokens=256, temperature=0.8):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "    \n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "        top_p=0.9,\n",
    "        repetition_penalty=1.1,\n",
    "        do_sample=True,\n",
    "    )\n",
    "    \n",
    "    return tokenizer.decode(\n",
    "        outputs[0][inputs.input_ids.shape[1]:],\n",
    "        skip_special_tokens=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Тестовые промпты\n",
    "test_prompts = [\n",
    "    \"Рубоӣ бинавис\",\n",
    "    \"Ғазали ошиқона эҷод кун\",\n",
    "    \"Шеър дар бораи баҳор бинавис\",\n",
    "]\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Запрос: {prompt}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(generate_poem(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Экспорт в GGUF (опционально)\n",
    "\n",
    "Для запуска на CPU через llama.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Экспорт в GGUF\n",
    "gguf_path = f\"{MODEL_DIR}/tajik-poetry-q4\"\n",
    "\n",
    "model.save_pretrained_gguf(\n",
    "    gguf_path,\n",
    "    tokenizer,\n",
    "    quantization_method=\"q4_k_m\",\n",
    ")\n",
    "print(f\"GGUF сохранён: {gguf_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\" + \"=\"*50)\nprint(\"Обучение завершено!\")\nprint(\"=\"*50)\nprint(f\"\\nМодели сохранены в Google Drive: {MODEL_DIR}\")\nprint(\"\\nСкачайте модель и используйте локально:\")\nprint(\"  python -m cli.generate --model путь/к/модели\")"
  }
 ]
}
{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# ShohnomaLLM - Обучение модели\n\nFine-tuning Qwen2.5-1.5B для генерации таджикских стихов.\n\n**Требования:**\n- GPU: T4 (15GB VRAM) или лучше\n- Данные в репозитории (клонируется автоматически)\n\n**Время обучения:** ~2-3 часа на T4, ~1 час на L4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проверка GPU\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Установка зависимостей\n",
    "%%capture\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install --no-deps trl peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Клонируем репозиторий ShohnomaLLM\n",
    "!git clone https://github.com/Kuchizu/ShohnomaLLM.git\n",
    "%cd ShohnomaLLM\n",
    "!pip install -r requirements.txt -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Подключение Google Drive (для сохранения модели)\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\n# Пути\nREPO_DIR = \"/content/ShohnomaLLM\"\nDATA_DIR = f\"{REPO_DIR}/data\"  # Данные из репозитория\nMODEL_DIR = \"/content/drive/MyDrive/ShohnomaLLM/models\"  # Модели в Drive\n\n# Создаём директории\n!mkdir -p {DATA_DIR}/processed {DATA_DIR}/training {MODEL_DIR}"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Подготовка данных\n",
    "\n",
    "Используем модули из репозитория"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nsys.path.insert(0, REPO_DIR)\n\nfrom training.format_dataset import DatasetFormatter\nfrom training.config import get_config\n\n# Загружаем конфигурацию\nconfig = get_config(\"colab_t4\")  # или \"colab_a100\" для A100\nprint(f\"Модель: {config.model.base_model}\")\nprint(f\"LoRA rank: {config.lora.r}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Форматируем датасет\nformatter = DatasetFormatter()\n\n# Путь к сырым данным (из склонированного репозитория)\nraw_data = f\"{DATA_DIR}/raw/ganjoor/all_classical.jsonl\"\n\n# Проверяем наличие данных\nimport os\nif os.path.exists(raw_data):\n    print(f\"Данные найдены: {raw_data}\")\n    \n    # Форматируем\n    processed_file = f\"{DATA_DIR}/processed/classical.jsonl\"\n    \n    count = formatter.process_jsonl(raw_data, processed_file, source_type=\"ganjoor\")\n    print(f\"Обработано: {count} примеров\")\n    \n    # Разбиваем на train/val\n    formatter.create_train_val_split(processed_file, f\"{DATA_DIR}/training\")\nelse:\n    print(f\"ОШИБКА: Данные не найдены по пути: {raw_data}\")\n    print(f\"Проверьте, что файл data/raw/ganjoor/all_classical.jsonl есть в репозитории\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Загрузка модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "# Загрузка модели\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=config.model.base_model,\n",
    "    max_seq_length=config.model.max_seq_length,\n",
    "    dtype=None,\n",
    "    load_in_4bit=config.model.load_in_4bit,\n",
    ")\n",
    "\n",
    "print(f\"Модель загружена: {config.model.base_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Добавляем LoRA\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=config.lora.r,\n",
    "    target_modules=config.lora.target_modules,\n",
    "    lora_alpha=config.lora.lora_alpha,\n",
    "    lora_dropout=config.lora.lora_dropout,\n",
    "    bias=config.lora.bias,\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "print(\"LoRA добавлен\")\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "# Загрузка датасета\n",
    "dataset = load_dataset(\n",
    "    'json',\n",
    "    data_files={\n",
    "        'train': f\"{DATA_DIR}/training/train.jsonl\",\n",
    "        'validation': f\"{DATA_DIR}/training/val.jsonl\",\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(dataset['train'])}\")\n",
    "print(f\"Val: {len(dataset['validation'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Trainer\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=dataset['train'],\n    eval_dataset=dataset['validation'],\n    dataset_text_field=\"text\",\n    max_seq_length=config.model.max_seq_length,\n    args=TrainingArguments(\n        output_dir=\"./outputs\",\n        per_device_train_batch_size=config.training.per_device_train_batch_size,\n        per_device_eval_batch_size=config.training.per_device_eval_batch_size,\n        gradient_accumulation_steps=config.training.gradient_accumulation_steps,\n        learning_rate=config.training.learning_rate,\n        lr_scheduler_type=config.training.lr_scheduler_type,\n        warmup_ratio=config.training.warmup_ratio,\n        num_train_epochs=config.training.num_train_epochs,\n        bf16=config.training.bf16,\n        optim=config.training.optim,\n        weight_decay=config.training.weight_decay,\n        max_grad_norm=config.training.max_grad_norm,\n        logging_steps=config.training.logging_steps,\n        eval_steps=config.training.eval_steps,\n        eval_strategy=config.training.evaluation_strategy,\n        save_steps=config.training.save_steps,\n        save_strategy=config.training.save_strategy,\n        save_total_limit=config.training.save_total_limit,\n        load_best_model_at_end=config.training.load_best_model_at_end,\n        seed=config.training.seed,\n        report_to=config.training.report_to,\n    ),\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Запуск обучения\n",
    "print(\"Начало обучения...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Сохранение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохраняем LoRA\n",
    "lora_path = f\"{MODEL_DIR}/tajik-poetry-lora\"\n",
    "model.save_pretrained(lora_path)\n",
    "tokenizer.save_pretrained(lora_path)\n",
    "print(f\"LoRA сохранён: {lora_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Объединяем LoRA с базовой моделью (16-bit)\n",
    "merged_path = f\"{MODEL_DIR}/tajik-poetry-1.5b\"\n",
    "\n",
    "model.save_pretrained_merged(\n",
    "    merged_path,\n",
    "    tokenizer,\n",
    "    save_method=\"merged_16bit\",\n",
    ")\n",
    "print(f\"Merged модель: {merged_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Тестирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Режим inference\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Системный промпт\n",
    "from training.format_dataset import SYSTEM_PROMPT\n",
    "\n",
    "def generate_poem(prompt, max_tokens=256, temperature=0.8):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "    \n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "        top_p=0.9,\n",
    "        repetition_penalty=1.1,\n",
    "        do_sample=True,\n",
    "    )\n",
    "    \n",
    "    return tokenizer.decode(\n",
    "        outputs[0][inputs.input_ids.shape[1]:],\n",
    "        skip_special_tokens=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Тестовые промпты\n",
    "test_prompts = [\n",
    "    \"Рубоӣ бинавис\",\n",
    "    \"Ғазали ошиқона эҷод кун\",\n",
    "    \"Шеър дар бораи баҳор бинавис\",\n",
    "]\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Запрос: {prompt}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(generate_poem(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Экспорт в GGUF (опционально)\n",
    "\n",
    "Для запуска на CPU через llama.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Экспорт в GGUF\n",
    "gguf_path = f\"{MODEL_DIR}/tajik-poetry-q4\"\n",
    "\n",
    "model.save_pretrained_gguf(\n",
    "    gguf_path,\n",
    "    tokenizer,\n",
    "    quantization_method=\"q4_k_m\",\n",
    ")\n",
    "print(f\"GGUF сохранён: {gguf_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\" + \"=\"*50)\nprint(\"Обучение завершено!\")\nprint(\"=\"*50)\nprint(f\"\\nМодели сохранены в Google Drive: {MODEL_DIR}\")\nprint(\"\\nСкачайте модель и используйте локально:\")\nprint(\"  python -m cli.generate --model путь/к/модели\")"
  }
 ]
}
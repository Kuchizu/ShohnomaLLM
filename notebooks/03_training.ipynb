{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# ShohnomaLLM - Обучение модели\n\nFine-tuning **Qwen3-4B** для генерации таджикских стихов.\n\n**Почему Qwen3:**\n- Qwen3-4B ≈ Qwen2.5-7B по качеству\n- Лучше reasoning и multilingual\n- 100+ языков включая персидский\n\n**Оптимизации для A100:**\n- ✅ Packing — 2-5x ускорение\n- ✅ Batch size 16\n- ✅ 4 dataloader workers\n\n**Время: ~30-60 минут**\n\n⚠️ Runtime → Change runtime type → **A100 GPU**"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проверка GPU\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Установка зависимостей\n",
    "%%capture\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install --no-deps trl peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Клонируем репозиторий ShohnomaLLM\n",
    "!git clone https://github.com/Kuchizu/ShohnomaLLM.git\n",
    "%cd ShohnomaLLM\n",
    "!pip install -r requirements.txt -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Подключение Google Drive (для сохранения модели)\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\n# Пути\nREPO_DIR = \"/content/ShohnomaLLM\"\nDATA_DIR = f\"{REPO_DIR}/data\"  # Данные из репозитория\nMODEL_DIR = \"/content/drive/MyDrive/ShohnomaLLM/models\"  # Модели в Drive\n\n# Создаём директории\n!mkdir -p {DATA_DIR}/processed {DATA_DIR}/training {MODEL_DIR}"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Подготовка данных\n",
    "\n",
    "Используем модули из репозитория"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nsys.path.insert(0, REPO_DIR)\n\nfrom training.format_dataset import DatasetFormatter\nfrom training.config import get_config\n\n# Загружаем конфигурацию для A100\nconfig = get_config(\"colab_a100\")\nprint(f\"Модель: {config.model.base_model}\")\nprint(f\"LoRA rank: {config.lora.r}\")\nprint(f\"Batch size: {config.training.per_device_train_batch_size}\")\nprint(f\"Gradient accumulation: {config.training.gradient_accumulation_steps}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Загрузка данных с HuggingFace (если нет локальных)\nimport os\nimport json\nfrom tqdm import tqdm\n\nlocal_data = f\"{DATA_DIR}/raw/ganjoor/all_classical.jsonl\"\nhf_data = f\"{DATA_DIR}/raw/ganjoor_hf/all_poems.jsonl\"\n\n# Если HF данных нет - скачиваем\nif not os.path.exists(hf_data):\n    print(\"Загрузка датасета с HuggingFace (119K стихов)...\")\n    from datasets import load_dataset\n    \n    # Создаём директорию\n    os.makedirs(f\"{DATA_DIR}/raw/ganjoor_hf\", exist_ok=True)\n    \n    # Загружаем датасет\n    dataset_hf = load_dataset(\"mabidan/ganjoor\", split=\"train\")\n    print(f\"Загружено: {len(dataset_hf)} стихов\")\n    \n    # Инициализируем транслитератор\n    from scraper.utils.transliterate import PersianToTajikTransliterator\n    transliterator = PersianToTajikTransliterator()\n    \n    # Обрабатываем и сохраняем\n    print(\"Транслитерация в таджикскую кириллицу...\")\n    with open(hf_data, 'w', encoding='utf-8') as f:\n        for item in tqdm(dataset_hf, desc=\"Обработка\"):\n            text = item.get(\"text\", \"\")\n            if not text or len(text) < 30:\n                continue\n            \n            text_tajik = transliterator.transliterate_poem(text)\n            \n            poem_data = {\n                \"id\": f\"hf_{item.get('id', 0)}\",\n                \"poet\": item.get(\"poet\", \"\"),\n                \"text_tajik\": text_tajik,\n                \"form\": \"other\",\n                \"source\": \"huggingface\",\n            }\n            json.dump(poem_data, f, ensure_ascii=False)\n            f.write('\\n')\n    \n    print(f\"Сохранено: {hf_data}\")\n\n# Подготовка датасета\nfrom training.format_dataset import prepare_full_dataset\n\nstats = prepare_full_dataset(\n    raw_dir=f\"{DATA_DIR}/raw\",\n    processed_dir=f\"{DATA_DIR}/processed\",\n    training_dir=f\"{DATA_DIR}/training\",\n)\nprint(f\"\\nВсего подготовлено: {stats.get('total', 0)} примеров\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Загрузка модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "# Загрузка модели\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=config.model.base_model,\n",
    "    max_seq_length=config.model.max_seq_length,\n",
    "    dtype=None,\n",
    "    load_in_4bit=config.model.load_in_4bit,\n",
    ")\n",
    "\n",
    "print(f\"Модель загружена: {config.model.base_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Добавляем LoRA (БЕЗ gradient checkpointing для скорости на A100)\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r=config.lora.r,\n    target_modules=config.lora.target_modules,\n    lora_alpha=config.lora.lora_alpha,\n    lora_dropout=config.lora.lora_dropout,\n    bias=config.lora.bias,\n    use_gradient_checkpointing=False,  # Выключено для A100 - быстрее!\n    random_state=42,\n)\n\nprint(\"LoRA добавлен\")\nmodel.print_trainable_parameters()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from datasets import load_dataset\n\n# Загрузка датасета\ndataset = load_dataset(\n    'json',\n    data_files={\n        'train': f\"{DATA_DIR}/training/train.jsonl\",\n        'validation': f\"{DATA_DIR}/training/val.jsonl\",\n    }\n)\n\nprint(f\"Train: {len(dataset['train'])}\")\nprint(f\"Val: {len(dataset['validation'])}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Настройка параметров - МАКСИМАЛЬНАЯ СКОРОСТЬ ДЛЯ A100\nfrom trl import SFTTrainer, SFTConfig\n\ndataset_size = len(dataset['train'])\nprint(f\"Размер датасета: {dataset_size}\")\n\n# Эпохи в зависимости от размера данных\nif dataset_size > 30000:\n    num_epochs = 2\n    eval_steps = 1000\n    save_steps = 1000\nelse:\n    num_epochs = 3\n    eval_steps = 500\n    save_steps = 500\n\nprint(f\"Epochs: {num_epochs}\")\nprint(f\"С packing ожидается ~50 минут обучения\")\n\n# Trainer с PACKING - 2-5x быстрее для коротких текстов!\ntrainer = SFTTrainer(\n    model=model,\n    processing_class=tokenizer,\n    train_dataset=dataset['train'],\n    eval_dataset=dataset['validation'],\n    \n    args=SFTConfig(\n        output_dir=\"./outputs\",\n        dataset_text_field=\"text\",  # Поле с текстом!\n        \n        # === PACKING - ГЛАВНАЯ ОПТИМИЗАЦИЯ ===\n        packing=True,\n        max_seq_length=1024,\n        dataset_num_proc=4,\n        \n        # === BATCH SIZE для A100 ===\n        per_device_train_batch_size=16,\n        per_device_eval_batch_size=16,\n        gradient_accumulation_steps=2,\n        \n        # === Learning Rate ===\n        learning_rate=2e-4,\n        lr_scheduler_type=\"cosine\",\n        warmup_ratio=0.05,\n        num_train_epochs=num_epochs,\n        \n        # === Precision ===\n        bf16=True,\n        optim=\"adamw_8bit\",\n        weight_decay=0.01,\n        max_grad_norm=1.0,\n        \n        # === Dataloader ===\n        dataloader_num_workers=4,\n        dataloader_pin_memory=True,\n        \n        # === Logging ===\n        logging_steps=50,\n        eval_steps=eval_steps,\n        eval_strategy=\"steps\",\n        save_steps=save_steps,\n        save_strategy=\"steps\",\n        save_total_limit=2,\n        load_best_model_at_end=True,\n        \n        seed=42,\n        report_to=\"none\",\n    ),\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Запуск обучения\n",
    "print(\"Начало обучения...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Сохранение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохраняем LoRA\n",
    "lora_path = f\"{MODEL_DIR}/tajik-poetry-lora\"\n",
    "model.save_pretrained(lora_path)\n",
    "tokenizer.save_pretrained(lora_path)\n",
    "print(f\"LoRA сохранён: {lora_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Объединяем LoRA с базовой моделью (16-bit)\nmerged_path = f\"{MODEL_DIR}/tajik-poetry-4b\"\n\nmodel.save_pretrained_merged(\n    merged_path,\n    tokenizer,\n    save_method=\"merged_16bit\",\n)\nprint(f\"Merged модель: {merged_path}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Тестирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Режим inference\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Системный промпт\n",
    "from training.format_dataset import SYSTEM_PROMPT\n",
    "\n",
    "def generate_poem(prompt, max_tokens=256, temperature=0.8):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "    \n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "        top_p=0.9,\n",
    "        repetition_penalty=1.1,\n",
    "        do_sample=True,\n",
    "    )\n",
    "    \n",
    "    return tokenizer.decode(\n",
    "        outputs[0][inputs.input_ids.shape[1]:],\n",
    "        skip_special_tokens=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Тестовые промпты\n",
    "test_prompts = [\n",
    "    \"Рубоӣ бинавис\",\n",
    "    \"Ғазали ошиқона эҷод кун\",\n",
    "    \"Шеър дар бораи баҳор бинавис\",\n",
    "]\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Запрос: {prompt}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(generate_poem(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Экспорт в GGUF (опционально)\n",
    "\n",
    "Для запуска на CPU через llama.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Экспорт в GGUF\n",
    "gguf_path = f\"{MODEL_DIR}/tajik-poetry-q4\"\n",
    "\n",
    "model.save_pretrained_gguf(\n",
    "    gguf_path,\n",
    "    tokenizer,\n",
    "    quantization_method=\"q4_k_m\",\n",
    ")\n",
    "print(f\"GGUF сохранён: {gguf_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\" + \"=\"*50)\nprint(\"Обучение завершено!\")\nprint(\"=\"*50)\nprint(f\"\\nМодели сохранены в Google Drive: {MODEL_DIR}\")\nprint(\"\\nСкачайте модель и используйте локально:\")\nprint(\"  python -m cli.generate --model путь/к/модели\")"
  }
 ]
}